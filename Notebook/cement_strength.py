# -*- coding: utf-8 -*-
"""cement strength.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wUr-DwY9q74t6stpilQhBxlfoRoENhwk

#IMPORTING LIBRARIES
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv("/content/concrete_dataas.csv")

"""#DATA UNDERSTANDING

Exploratory Data Analysis
"""

df.shape

df.head()

#checking for null values
df.isnull().sum()

df.describe()

df.info()

df.duplicated().sum()

dropped=df.drop_duplicates(inplace = True)

# checking for outliers

variables = df.columns

plt.figure(figsize = (10, 8))
plotnumber = 1

for i in range(1, len(variables)):
    if plotnumber <= 9:
        ax = plt.subplot(3, 3, plotnumber)
        sns.boxplot(y = variables[i], data = df, ax = ax)
        plt.title(f"\n{variables[i]} \n", fontsize = 10)

    plotnumber += 1

plt.tight_layout()
plt.show()

#Treating the Outliers
from scipy.stats.mstats import winsorize
cols='Water','Age','Superplasticizer'
for column in cols:
    df[column] = winsorize(df[column], limits=[0.05, 0.05])

# rechecking for the otliers

plt.figure(figsize = (10, 8))
plotnumber = 1

for i in range(1, len(variables)):
    if plotnumber <= 9:
        ax = plt.subplot(3, 3, plotnumber)
        sns.boxplot(y = variables[i], data = df, ax = ax)
        plt.title(f"\n{variables[i]} \n", fontsize = 10)

    plotnumber += 1

plt.tight_layout()
plt.show()

histogram = df.hist(figsize = (10, 12))

"""Cement:

The distribution is right-skewed with a concentration between 100 and 300.
There are fewer instances of higher cement amounts.

Blast Furnace Slag:

Highly right-skewed with a significant number of instances at 0.
Very few instances of high values, indicating many samples do not use blast furnace slag.

Fly Ash:

Similar to Blast Furnace Slag, highly right-skewed with many instances at 0.
Indicates many samples do not contain fly ash.

Water:

Slightly left-skewed with a concentration around 180-220.
Most samples have a consistent amount of water.
Superplasticizer:

Highly right-skewed with a significant number of instances at 0.
Few samples use high amounts of superplasticizer.

Coarse Aggregate:

Fairly normally distributed with a peak around 1000.

Fine Aggregate:

Distribution is slightly left-skewed with a peak around 800.

Age:

Highly right-skewed with many samples aged 0-50 days.
Few instances of high age values.

Strength:

Appears normally distributed with a peak around 20-40.

CORRELATION
"""

plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.show()

"""Cement and Strength:

There is a positive correlation (0.49) between Cement and Strength, indicating that as the amount of cement increases, the strength of the concrete also tends to increase.

Age and Strength:

Age has a positive correlation (0.42) with Strength, suggesting that the strength of the concrete increases as it ages.

Superplasticizer and Strength:

There is a moderate positive correlation (0.33) between Superplasticizer and Strength, indicating that using superplasticizers can improve the strength of the concrete.

Water and Strength:

There is a negative correlation (-0.27) between Water and Strength, which means that as the amount of water increases, the strength of the concrete tends to decrease.

Fly Ash and Strength:

Fly Ash shows a weak negative correlation (-0.081) with Strength.

Other Ingredients and Strength:

Blast Furnace Slag (0.1), Coarse Aggregate (-0.14), and Fine Aggregate (-0.19) show weaker correlations with Strength.

#TRAIN TEST SPLIT
"""

x = df.drop(['Strength'], axis=1)

y = df['Strength']

print(x.shape)
print(y.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25,random_state=9)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

skewness=X_train.skew()
print(skewness)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
x_train = sc.fit_transform(X_train)
x_test = sc.fit_transform(X_test)

"""#MODEL BUILDING"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

from sklearn.model_selection import cross_val_score

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'Elastic Net': ElasticNet(),
    'Decision Tree': DecisionTreeRegressor(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'XGBoost': XGBRegressor(),

}

# Train and evaluate each model
results = {}
for name, model in models.items():
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
    model.fit(x_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    results[name] = {'RMSE': -cv_scores.mean(), 'MSE': mse, 'R2': r2, 'MAE': mae }


# Display the results
results_df = pd.DataFrame(results).T
print(results_df)

"""#HYPER PARAMETER TUNING"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Best model after hyperparameter tuning
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)
best_mse = mean_squared_error(y_test, y_pred_best)
best_r2 = r2_score(y_test, y_pred_best)
best_mae = mean_absolute_error(y_test, y_pred)

print(f'Best Model Test RMSE: {np.sqrt(best_mse)}')
print(f'Best Model R2 Score: {best_r2}')
print(f'Best Model MAE: {best_mae}')

best_model_name = results_df.sort_values(by='RMSE').index[0]
print(f'The best model is: {best_model_name} with RMSE: {results_df.loc[best_model_name, "RMSE"]}')

import pickle

best_model = best_model_name
with open('best_model.pkl', 'wb') as file:
    pickle.dump(best_model, file)

print("Best model saved as 'best_model.pkl'")